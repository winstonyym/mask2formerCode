{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd235711-ee37-44d7-8570-8cefe39a1420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.cuda import OutOfMemoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87eb423-06ee-4c44-b5b0-41cfd7d4ec7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((120, 120))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1aec3f5-f597-47e3-a4f2-9b7b9db7c0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 65\n",
    "CLS_DICT = {'0': 'Bird',\n",
    " '1': 'Ground-Animal',\n",
    " '2': 'Curb',\n",
    " '3': 'Fence',\n",
    " '4': 'Guard-Rail',\n",
    " '5': 'Barrier',\n",
    " '6': 'Wall',\n",
    " '7': 'Bike-Lane',\n",
    " '8': 'Crosswalk---Plain',\n",
    " '9': 'Curb-Cut',\n",
    " '10': 'Parking',\n",
    " '11': 'Pedestrian-Area',\n",
    " '12': 'Rail-Track',\n",
    " '13': 'Road',\n",
    " '14': 'Service-Lane',\n",
    " '15': 'Sidewalk',\n",
    " '16': 'Bridge',\n",
    " '17': 'Building',\n",
    " '18': 'Tunnel',\n",
    " '19': 'Person',\n",
    " '20': 'Bicyclist',\n",
    " '21': 'Motorcyclist',\n",
    " '22': 'Other-Rider',\n",
    " '23': 'Lane-Marking---Crosswalk',\n",
    " '24': 'Lane-Marking---General',\n",
    " '25': 'Mountain',\n",
    " '26': 'Sand',\n",
    " '27': 'Sky',\n",
    " '28': 'Snow',\n",
    " '29': 'Terrain',\n",
    " '30': 'Vegetation',\n",
    " '31': 'Water',\n",
    " '32': 'Banner',\n",
    " '33': 'Bench',\n",
    " '34': 'Bike-Rack',\n",
    " '35': 'Billboard',\n",
    " '36': 'Catch-Basin',\n",
    " '37': 'CCTV-Camera',\n",
    " '38': 'Fire-Hydrant',\n",
    " '39': 'Junction-Box',\n",
    " '40': 'Mailbox',\n",
    " '41': 'Manhole',\n",
    " '42': 'Phone-Booth',\n",
    " '43': 'Pothole',\n",
    " '44': 'Street-Light',\n",
    " '45': 'Pole',\n",
    " '46': 'Traffic-Sign-Frame',\n",
    " '47': 'Utility-Pole',\n",
    " '48': 'Traffic-Light',\n",
    " '49': 'Traffic-Sign-(Back)',\n",
    " '50': 'Traffic-Sign-(Front)',\n",
    " '51': 'Trash-Can',\n",
    " '52': 'Bicycle',\n",
    " '53': 'Boat',\n",
    " '54': 'Bus',\n",
    " '55': 'Car',\n",
    " '56': 'Caravan',\n",
    " '57': 'Motorcycle',\n",
    " '58': 'On-Rails',\n",
    " '59': 'Other-Vehicle',\n",
    " '60': 'Trailer',\n",
    " '61': 'Truck',\n",
    " '62': 'Wheeled-Slow',\n",
    " '63': 'Car-Mount',\n",
    " '64': 'Ego-Vehicle'}\n",
    "\n",
    "# Get helper function\n",
    "def addIndice(output_max):\n",
    "    set_of_pixels = torch.unique(output_max, return_counts=True)\n",
    "    set_dictionary = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "            set_dictionary[str(i)] = 0\n",
    "    for pixel,count in zip(set_of_pixels[0], set_of_pixels[1]):\n",
    "        set_dictionary[str(pixel.item())] = count.item()\n",
    "    set_dictionary['Total'] = int(np.sum(list(set_dictionary.values())))\n",
    "    return set_dictionary\n",
    "\n",
    "def addInstance(output_max):\n",
    "    list_unique, list_counts = torch.unique(out[0]['segmentation'].int(), return_counts=True)\n",
    "\n",
    "    if -1 in list_unique:\n",
    "        list_unique = list_unique[1:]\n",
    "        list_counts = list_counts[1:]\n",
    "\n",
    "    total = torch.sum(list_counts).item()\n",
    "\n",
    "    matching_dict = {}\n",
    "    for i, k in zip(range(len(out[0]['segments_info'])), out[0]['segments_info']):\n",
    "        matching_dict[i] = int(k['label_id'])\n",
    "\n",
    "    set_dictionary = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "                set_dictionary[str(i)] = 0\n",
    "\n",
    "    for i, k in zip(list_unique, list_counts):\n",
    "        set_dictionary[str(matching_dict[i.item()])] += k.item()\n",
    "        \n",
    "    set_dictionary['Total'] = total\n",
    "\n",
    "    return set_dictionary\n",
    "\n",
    "def addInstanceCounts(output_max):\n",
    "\n",
    "    instance_dictionary = {}\n",
    "    \n",
    "    instance_dictionary = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "                instance_dictionary[str(i)] = 0\n",
    "    \n",
    "    # for each segment, draw its legend\n",
    "    for segment in out[0]['segments_info']:\n",
    "        segment_id = segment['id']\n",
    "        segment_label_id = str(segment['label_id'])\n",
    "        instance_dictionary[segment_label_id] += 1\n",
    "\n",
    "    return instance_dictionary\n",
    "\n",
    "# Load Mask2Former\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-mapillary-vistas-panoptic\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-mapillary-vistas-panoptic\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ec6e03-1fed-48b8-86fa-b260a9736dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a27de90-9e34-4d19-9444-4da19b882d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set start state\n",
    "image_indicators_dict = {}\n",
    "image_instances_dict = {}\n",
    "\n",
    "# Create output folder if none exist\n",
    "if not os.path.exists(f'./outputs'):\n",
    "    os.makedirs(f'./outputs')\n",
    "\n",
    "# Get list of images\n",
    "image_set = [i for i in os.listdir(os.path.join(os.getcwd(),'/media/ual/UAL-PSSD-1/global_streetscapes_imgs_1/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b94224-096b-49db-989b-66d0a983e857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677722"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c211a209-514c-4305-af6d-bbb71474493f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                          | 0/1677722 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 278f7c8a-3063-4acd-ae93-4c6ff848c8d3.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ual/.conda/envs/svi_global/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "  0%|                                                                                                                                                             | 1/1677722 [00:04<1893:00:39,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting b6a4d5a1-dde2-40c9-9c55-bd6fb3431bef.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 2/1677722 [00:07<1620:57:37,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting c502f3d8-0c08-407f-b2e1-14f3d5de49b6.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 3/1677722 [00:10<1495:58:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 77536944-fd4e-4f0d-a724-a2d54f47b9a9.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 4/1677722 [00:13<1526:58:00,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting fb8768bc-d9f6-4618-8dcb-7442a5337a5d.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 5/1677722 [00:16<1473:39:06,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 5cff3614-fbd7-4f9e-a36b-2e6acc35a6b2.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 6/1677722 [00:19<1422:06:17,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 507ff643-59a9-4f2c-97c8-5a8e4b3cc77c.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 7/1677722 [00:22<1472:00:50,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 78bb3185-5157-43a8-8e42-971c2da46186.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 8/1677722 [00:25<1503:46:39,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 27413128-043e-4f89-b135-0c2249677d1b.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 9/1677722 [00:28<1455:15:40,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting d8494c7a-eb8f-4fea-854e-921512ecd609.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 10/1677722 [00:31<1418:07:34,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 1f08f941-1baf-460f-bc0d-672a29325db5.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 11/1677722 [00:34<1415:11:29,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 10b32d5f-84d7-486e-9c6b-7fe9b64385b0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 12/1677722 [00:37<1388:56:23,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 6f032d01-f078-4c4c-b2dc-92b8ed5da2f1.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 12/1677722 [00:38<1477:26:33,  3.17s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.18 GiB (GPU 0; 7.92 GiB total capacity; 1.17 GiB already allocated; 5.75 GiB free; 1.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(pixel_values \u001b[38;5;241m=\u001b[39m pixel_values, pixel_mask \u001b[38;5;241m=\u001b[39m pixel_mask)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# outputs['class_queries_logits'] = outputs['class_queries_logits'].to('cpu')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# outputs['masks_queries_logits'] = outputs['masks_queries_logits'].to('cpu')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# outputs['encoder_last_hidden_state'] = outputs['encoder_last_hidden_state'].to('cpu')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# outputs['pixel_decoder_last_hidden_state'] = outputs['pixel_decoder_last_hidden_state'].to('cpu')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# outputs['transformer_decoder_last_hidden_state'] = outputs['transformer_decoder_last_hidden_state'].to('cpu')\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process_instance_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m image_indicators_dict[image] \u001b[38;5;241m=\u001b[39m addInstance(out)\n\u001b[1;32m     16\u001b[0m image_instances_dict[image] \u001b[38;5;241m=\u001b[39m addInstanceCounts(out)\n",
      "File \u001b[0;32m~/.conda/envs/svi_global/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:1024\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.post_process_instance_segmentation\u001b[0;34m(self, outputs, threshold, mask_threshold, overlap_mask_area_threshold, target_sizes, return_coco_annotation, return_binary_maps)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     segmentation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(target_sizes[i]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1024\u001b[0m     pred_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1028\u001b[0m instance_maps, segments \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m   1029\u001b[0m current_segment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/svi_global/lib/python3.9/site-packages/torch/nn/functional.py:3931\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[1;32m   3930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.18 GiB (GPU 0; 7.92 GiB total capacity; 1.17 GiB already allocated; 5.75 GiB free; 1.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i, image in enumerate(tqdm(image_set[0:])):\n",
    "    print(f'Segmenting {image}')\n",
    "    img = Image.open(os.path.join(os.getcwd(),'/media/ual/UAL-PSSD-1/global_streetscapes_imgs_1/', f'{image}'))\n",
    "    inputs = processor(images=img, return_tensors=\"pt\", height = 120, width=120)\n",
    "    with torch.no_grad():\n",
    "        pixel_values = inputs['pixel_values'].to(device)\n",
    "        pixel_mask = inputs['pixel_mask'].to(device)\n",
    "        outputs = model(pixel_values = pixel_values, pixel_mask = pixel_mask)\n",
    "        outputs['class_queries_logits'] = outputs['class_queries_logits'].to('cpu')\n",
    "        outputs['masks_queries_logits'] = outputs['masks_queries_logits'].to('cpu')\n",
    "        outputs['encoder_last_hidden_state'] = outputs['encoder_last_hidden_state'].to('cpu')\n",
    "        outputs['pixel_decoder_last_hidden_state'] = outputs['pixel_decoder_last_hidden_state'].to('cpu')\n",
    "        outputs['transformer_decoder_last_hidden_state'] = outputs['transformer_decoder_last_hidden_state'].to('cpu')\n",
    "    out = processor.post_process_instance_segmentation(outputs, target_sizes=[img.size[::-1]], threshold=0.25)\n",
    "    image_indicators_dict[image] = addInstance(out)\n",
    "    image_instances_dict[image] = addInstanceCounts(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c104650-b12e-4a4e-aa9a-264beff62940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c130a-b04e-4f21-b317-9d24521ef709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Segmenting {image}')\n",
    "img = Image.open(os.path.join(os.getcwd(),'/media/ual/UAL-PSSD-1/global_streetscapes_imgs_1/', '6f032d01-f078-4c4c-b2dc-92b8ed5da2f1.jpeg'))\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    pixel_mask = inputs['pixel_mask'].to(device)\n",
    "    outputs = model(pixel_values = pixel_values, pixel_mask = pixel_mask)\n",
    "    outputs['class_queries_logits'] = outputs['class_queries_logits'].to('cpu')\n",
    "    outputs['masks_queries_logits'] = outputs['masks_queries_logits'].to('cpu')\n",
    "    outputs['encoder_last_hidden_state'] = outputs['encoder_last_hidden_state'].to('cpu')\n",
    "    outputs['pixel_decoder_last_hidden_state'] = outputs['pixel_decoder_last_hidden_state'].to('cpu')\n",
    "    outputs['transformer_decoder_last_hidden_state'] = outputs['transformer_decoder_last_hidden_state'].to('cpu')\n",
    "out = processor.post_process_instance_segmentation(outputs, target_sizes=[img.size[::-1]], threshold=0.25)\n",
    "image_indicators_dict[image] = addInstance(out)\n",
    "image_instances_dict[image] = addInstanceCounts(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f4d18-34b7-4ad6-b521-ef4cf1f9e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0] = outputs[0].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e31f28-bef8-451d-a193-e10849a12d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06d846-f75f-4cba-a621-ee0fd1130c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3320c1-19c7-4589-8868-e55b7c30ff0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bf947-ffff-4a4a-a7a5-91b45a2799a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(image_instances_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27774479-159c-4037-afa5-22a45ef884e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_instances_dict['139784358078458.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa3168-21bd-4826-9f85-87d44edfa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(os.getcwd(),'test/', '139784358078458.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a2cc8-c830-4148-a9e4-482d950f8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc026ab9-83b1-4254-99e0-7aa7d743b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1d142-00e3-49e8-a428-9ea5cf604bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out[0]['segmentation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c76f9-16ea-4015-bc99-32fd72bd4ce7",
   "metadata": {},
   "source": [
    "### Merge CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31010c69-1048-4de2-9c13-a738033f1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733cdbe-672f-46ab-9ac0-66fc0d286c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e1bb3-0d9e-43ef-b0b8-f51546e86791",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.read_csv(paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa54ee-d9b0-4cd9-93ad-908c9248c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe124b-cea4-46a8-8e61-15e4cb656d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([total, out2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1e7db-3c4a-4ba8-8769-aabf33cc6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(filepath):\n",
    "\n",
    "    extension = filepath + '*.csv'\n",
    "    paths = glob(extension)\n",
    "    \n",
    "    combined = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        temp = pd.read_csv(path)\n",
    "        combined = pd.concat([combined, temp], axis=0)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c23b1-3f6f-4e49-b62c-11fc99ef7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = merge_csv('./outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b804de1-4dc2-4c28-8f0a-d319b773a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a6c60-e103-4a5d-9db2-a6c957e799a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('./final/ssd1_1676060.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec53ef0-ca36-4c1a-945d-0288bc8fd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing log file\n",
    "\n",
    "with open('yourlog.log', 'w'):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svi_global",
   "language": "python",
   "name": "svi_global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
